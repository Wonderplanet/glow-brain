#!/usr/bin/env python3
"""
CSVãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦master_tables_schema.jsonã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    python scripts/filter_master_tables_schema.py [--dry-run]
"""

import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple


def camel_to_snake(name: str) -> str:
    """
    ã‚¢ãƒƒãƒ‘ãƒ¼ã‚­ãƒ£ãƒ¡ãƒ«ã‚±ãƒ¼ã‚¹ã‚’ã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ã«å¤‰æ›ã™ã‚‹
    ä¾‹: MstAdventBattle -> mst_advent_battle
    """
    # é€£ç¶šã™ã‚‹å¤§æ–‡å­—ã‚’å‡¦ç†ï¼ˆä¾‹: I18n -> i18nï¼‰
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    # å°æ–‡å­—ã®å¾Œã®å¤§æ–‡å­—ã‚’å‡¦ç†
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()


def get_csv_files(masterdata_dir: Path) -> List[str]:
    """
    glow-masterdataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’å–å¾—ã™ã‚‹
    """
    csv_files = []
    for csv_path in masterdata_dir.glob('*.csv'):
        # æ‹¡å¼µå­ã‚’é™¤å»
        csv_name = csv_path.stem
        csv_files.append(csv_name)
    return sorted(csv_files)


def pluralize(word: str) -> List[str]:
    """
    å˜æ•°å½¢ã®å˜èªã‚’è¤‡æ•°å½¢ã«å¤‰æ›ã™ã‚‹å€™è£œã‚’ç”Ÿæˆã™ã‚‹

    Args:
        word: å˜æ•°å½¢ã®å˜èª

    Returns:
        è¤‡æ•°å½¢ã®å€™è£œãƒªã‚¹ãƒˆ
    """
    candidates = []

    # 1. ãã®ã¾ã¾ï¼ˆã™ã§ã«è¤‡æ•°å½¢ã®å ´åˆï¼‰
    candidates.append(word)

    # 2. æœ«å°¾ã« 's' ã‚’è¿½åŠ 
    candidates.append(f"{word}s")

    # 3. æœ«å°¾ãŒ 'y' ã§çµ‚ã‚ã‚‹å ´åˆã€'ies' ã«å¤‰æ›
    if word.endswith('y'):
        candidates.append(f"{word[:-1]}ies")

    # 4. æœ«å°¾ãŒ 'x', 's', 'ch', 'sh' ã§çµ‚ã‚ã‚‹å ´åˆã€'es' ã‚’è¿½åŠ 
    if word.endswith(('x', 's', 'ch', 'sh')):
        candidates.append(f"{word}es")

    # 5. æœ«å°¾ãŒ 'fe' ã§çµ‚ã‚ã‚‹å ´åˆã€'ves' ã«å¤‰æ›
    if word.endswith('fe'):
        candidates.append(f"{word[:-2]}ves")

    # 6. æœ«å°¾ãŒ 'f' ã§çµ‚ã‚ã‚‹å ´åˆã€'ves' ã«å¤‰æ›
    if word.endswith('f') and not word.endswith('ff'):
        candidates.append(f"{word[:-1]}ves")

    return candidates


def csv_name_to_table_candidates(csv_name: str) -> List[str]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åå€™è£œã‚’ç”Ÿæˆã™ã‚‹

    Args:
        csv_name: CSVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ä¾‹: MstAdventBattle, MstAdventBattleI18n

    Returns:
        å€™è£œã¨ãªã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒªã‚¹ãƒˆï¼ˆã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ãƒ»è¤‡æ•°å½¢ï¼‰
    """
    # I18nã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’æŒã¤ã‹ãƒã‚§ãƒƒã‚¯
    has_i18n_suffix = csv_name.endswith('I18n')

    if has_i18n_suffix:
        # I18nã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
        base_name = csv_name[:-4]  # 'I18n' ã‚’é™¤å»
        # ãƒ™ãƒ¼ã‚¹åã‚’ã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ã«å¤‰æ›
        snake_base = camel_to_snake(base_name)
        # ãƒ™ãƒ¼ã‚¹åã‚’è¤‡æ•°å½¢ã«å¤‰æ›ã—ã¦ã€_i18n ã‚’è¿½åŠ 
        base_plurals = pluralize(snake_base)
        candidates = [f"{plural}_i18n" for plural in base_plurals]
    else:
        # é€šå¸¸ã®ãƒ†ãƒ¼ãƒ–ãƒ«åå‡¦ç†
        snake_name = camel_to_snake(csv_name)
        candidates = pluralize(snake_name)

    return candidates


def match_csv_to_table(csv_name: str, table_names: Set[str]) -> Tuple[str, str]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«åã‚’JSONãƒ†ãƒ¼ãƒ–ãƒ«åã«ãƒãƒƒãƒãƒ³ã‚°ã™ã‚‹

    Args:
        csv_name: CSVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        table_names: JSONã«å«ã¾ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã®ã‚»ãƒƒãƒˆ

    Returns:
        (csv_name, matched_table_name) ã¾ãŸã¯ (csv_name, None)
    """
    candidates = csv_name_to_table_candidates(csv_name)

    for candidate in candidates:
        if candidate in table_names:
            return (csv_name, candidate)

    return (csv_name, None)


def filter_schema(
    input_json_path: Path,
    masterdata_dir: Path,
    output_json_path: Path,
    dry_run: bool = False
) -> None:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦ã‚¹ã‚­ãƒ¼ãƒJSONã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
    """
    print("=" * 80)
    print("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ãJSONã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
    print("=" * 80)
    print()

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    print(f"ğŸ“– JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {input_json_path}")
    with open(input_json_path, 'r', encoding='utf-8') as f:
        schema_data = json.load(f)

    # ãƒ†ãƒ¼ãƒ–ãƒ«åä¸€è¦§ã‚’å–å¾—
    tables = schema_data.get('databases', {}).get('mst', {}).get('tables', {})
    table_names = set(tables.keys())
    print(f"   JSONã«å«ã¾ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_names)}")
    print()

    # CSVãƒ•ã‚¡ã‚¤ãƒ«åä¸€è¦§ã‚’å–å¾—
    print(f"ğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­: {masterdata_dir}")
    csv_files = get_csv_files(masterdata_dir)
    print(f"   è¦‹ã¤ã‹ã£ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")
    print()

    # ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†
    print("ğŸ” ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†é–‹å§‹")
    print("-" * 80)

    matched_tables = {}
    unmatched_csvs = []

    for csv_name in csv_files:
        csv_name_display, matched_table = match_csv_to_table(csv_name, table_names)

        if matched_table:
            matched_tables[matched_table] = csv_name_display
            print(f"âœ… {csv_name_display:50s} â†’ {matched_table}")
        else:
            unmatched_csvs.append(csv_name_display)
            print(f"âŒ {csv_name_display:50s} â†’ (ãƒãƒƒãƒãªã—)")

    print("-" * 80)
    print()

    # çµæœã‚µãƒãƒªãƒ¼
    print("ğŸ“Š ãƒãƒƒãƒãƒ³ã‚°çµæœ")
    print(f"   ãƒãƒƒãƒã—ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«: {len(matched_tables)}/{len(csv_files)}")
    print(f"   ãƒãƒƒãƒã—ãªã‹ã£ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«: {len(unmatched_csvs)}/{len(csv_files)}")
    print()

    if unmatched_csvs:
        print("âš ï¸  ãƒãƒƒãƒã—ãªã‹ã£ãŸCSVãƒ•ã‚¡ã‚¤ãƒ«:")
        for csv_name in unmatched_csvs:
            print(f"   - {csv_name}")
        print()

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ã“ã“ã§çµ‚äº†
    if dry_run:
        print("ğŸƒ ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œæˆã•ã‚Œã¾ã›ã‚“")
        print()
        return

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã®ã‚¹ã‚­ãƒ¼ãƒã‚’ä½œæˆ
    filtered_tables = {
        table_name: tables[table_name]
        for table_name in matched_tables.keys()
    }

    filtered_schema = {
        'databases': {
            'mst': {
                'tables': filtered_tables
            }
        }
    }

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
    print(f"ğŸ’¾ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿JSONã‚’å‡ºåŠ›ä¸­: {output_json_path}")
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(filtered_schema, f, ensure_ascii=False, indent=4)

    print(f"   å‡ºåŠ›ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(filtered_tables)}")
    print()
    print("âœ¨ å®Œäº†ã—ã¾ã—ãŸï¼")
    print()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # å¼•æ•°ãƒã‚§ãƒƒã‚¯
    dry_run = '--dry-run' in sys.argv

    # ãƒ‘ã‚¹ã®è¨­å®š
    script_dir = Path(__file__).parent
    project_root = script_dir.parent

    input_json_path = project_root / 'projects/glow-server/api/database/schema/exports/master_tables_schema.json'
    masterdata_dir = project_root / 'projects/glow-masterdata'
    output_json_path = project_root / 'projects/glow-server/api/database/schema/exports/master_tables_schema_filtered.json'

    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not input_json_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_json_path}", file=sys.stderr)
        sys.exit(1)

    if not masterdata_dir.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: masterdataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {masterdata_dir}", file=sys.stderr)
        sys.exit(1)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†å®Ÿè¡Œ
    filter_schema(input_json_path, masterdata_dir, output_json_path, dry_run)


if __name__ == '__main__':
    main()
